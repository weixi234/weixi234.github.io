<!DOCTYPE html>



  


<html class="theme-next mist use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">






  
  
  <link rel="stylesheet" media="all" href="/lib/Han/dist/han.min.css?v=3.3">




<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  
    
      
    

    
  

  

  
    
      
    

    
  

  
    
      
    

    
  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Monda:300,300italic,400,400italic,700,700italic|Roboto Slab:300,300italic,400,400italic,700,700italic|Lobster Two:300,300italic,400,400italic,700,700italic|PT Mono:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Caffe,Ubuntu," />










<meta name="description" content="前言去年刚开始搞RobotMaster比赛时接触过DeepLearning，当时在pyCharm中用python自己一行一行的从底层实现了mnist手写字符识别，基本上了解了深度学习的原理及实现方法。后来觉得深度学习也不过是编写程序，连深度学习底层都是用代码去实现一些数学关系，说到底是数学、是编程。我转而去研究去学习C/C++了，这一年多我自学了C、C++、STL两边，自学了模式设计UML，自学了">
<meta name="keywords" content="Caffe,Ubuntu">
<meta property="og:type" content="article">
<meta property="og:title" content="Caffe学习——1 跑LeNet mnist数据集">
<meta property="og:url" content="http://yoursite.com/2018/12/22/Caffe学习——1-跑LeNet-mnist数据集/index.html">
<meta property="og:site_name" content="未 晞">
<meta property="og:description" content="前言去年刚开始搞RobotMaster比赛时接触过DeepLearning，当时在pyCharm中用python自己一行一行的从底层实现了mnist手写字符识别，基本上了解了深度学习的原理及实现方法。后来觉得深度学习也不过是编写程序，连深度学习底层都是用代码去实现一些数学关系，说到底是数学、是编程。我转而去研究去学习C/C++了，这一年多我自学了C、C++、STL两边，自学了模式设计UML，自学了">
<meta property="og:locale" content="zh-Hans">
<meta property="og:updated_time" content="2018-12-23T13:28:22.121Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Caffe学习——1 跑LeNet mnist数据集">
<meta name="twitter:description" content="前言去年刚开始搞RobotMaster比赛时接触过DeepLearning，当时在pyCharm中用python自己一行一行的从底层实现了mnist手写字符识别，基本上了解了深度学习的原理及实现方法。后来觉得深度学习也不过是编写程序，连深度学习底层都是用代码去实现一些数学关系，说到底是数学、是编程。我转而去研究去学习C/C++了，这一年多我自学了C、C++、STL两边，自学了模式设计UML，自学了">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Mist',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":true},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/12/22/Caffe学习——1-跑LeNet-mnist数据集/"/>





  <title>Caffe学习——1 跑LeNet mnist数据集 | 未 晞</title>
  





  <script type="text/javascript">
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?294d40586178dce17bb24335bf3ca589";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">未 晞</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">白露未晞，看书学习到天明~</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-guestbook">
          <a href="/guestbook/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-fa fa-address-book"></i> <br />
            
            留言板
          </a>
        </li>
      

      
    </ul>
  

  
</nav>




 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/12/22/Caffe学习——1-跑LeNet-mnist数据集/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="未晞">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/tuxiang.png">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="未 晞">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Caffe学习——1 跑LeNet mnist数据集</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-12-22T16:37:14+08:00">
                2018-12-22
              </time>
            

            
              <span class="post-meta-divider">|</span>
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-check-o"></i>
              </span>
              
                <span class="post-meta-item-text">更新于&#58;</span>
              
              <time title="更新于" itemprop="dateModified" datetime="2018-12-23T21:28:22+08:00">
                2018-12-23
              </time>
            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2018/12/22/Caffe学习——1-跑LeNet-mnist数据集/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count gitment-comments-count" data-xid="/2018/12/22/Caffe学习——1-跑LeNet-mnist数据集/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <h1 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h1><p>去年刚开始搞RobotMaster比赛时接触过DeepLearning，当时在pyCharm中用python自己一行一行的从底层实现了mnist手写字符识别，基本上了解了深度学习的原理及实现方法。后来觉得深度学习也不过是编写程序，连深度学习底层都是用代码去实现一些数学关系，说到底是数学、是编程。<br>我转而去研究去学习C/C++了，这一年多我自学了C、C++、STL两边，自学了模式设计UML，自学了MFC、Qt等等。当然也写了一些程序跟着老师写了些项目，自认为也是有了一些编程基础了。现在开始写些C/CPP编程基础总结、开始研究学习深度学习了。（20181222）   </p>
<h1 id="Caffe及LeNetMnist相关说明"><a href="#Caffe及LeNetMnist相关说明" class="headerlink" title="Caffe及LeNetMnist相关说明"></a>Caffe及LeNetMnist相关说明</h1><p>下载下来的源码我放在    </p>
<blockquote>
<p>/home/zhouyang/caffe-master</p>
</blockquote>
<p>官方说明文档中将这一路径定义为CAFFE_ROOT。<br>关于mnist手写体识别其说明文档在 \$CAFFE_ROOT/examples/mnist/Readme.md       </p>
<p>Caffe中一般把数据源放在 \$CAFFE_ROOT/data文件夹下面；处理后的数据和模型文件等放在 \$CAFFE_ROOT/examples文件夹下。</p>
<h1 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h1><p>step1:获取mnist数据集脚本地址  </p>
<blockquote>
<p>/home/zhouyang/caffe-master/data/mnist   </p>
</blockquote>
<p>step2:运行脚本  </p>
<blockquote>
<p>cd $CAFFE_ROOT<br>./data/mnist/get_mnist.sh  </p>
</blockquote>
<p>这样在data目录下就会得到测试图像、测试标签、训练数据集图像和训练数据集标签四个文件。  </p>
<p>step3:转换格式为LMDB   </p>
<blockquote>
<p>cd $CAFFE_ROOT<br>./examples/mnist/create_mnist.sh<br>转换完成后在./examples/mnist目录下会生成mnist_train_lmdb和mnist_test_lmdb两个文件夹。  </p>
</blockquote>
<h1 id="配置网络模型"><a href="#配置网络模型" class="headerlink" title="配置网络模型"></a>配置网络模型</h1><blockquote>
<p>“Before we actually run the training program, let’s explain what will happen. We will use the <a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf" target="_blank" rel="noopener"><strong>LeNet论文</strong></a> network, which is known to work well on digit classification tasks. We will use a slightly different version from the original LeNet implementation, replacing the sigmoid activations with Rectified Linear Unit (ReLU) activations for the neurons.<br>The design of LeNet contains the essence of CNNs that are still used in larger models such as the ones in ImageNet. In general, it consists of a convolutional layer followed by a pooling layer, another convolution layer followed by a pooling layer, and then two fully connected layers similar to the conventional multilayer perceptrons. We have defined the layers in $CAFFE_ROOT/examples/mnist/lenet_train_test.prototxt.”<br>       ———— from readme   </p>
</blockquote>
<p>官网提供了定义好的网络文件 $CAFFE_ROOT/examples/mnist/lenet_train_test.prototxt </p>
<p>可以用官方自带的python绘图工具绘制出网络图：<br>在$CAFFE_ROOT/python下有draw_net.py脚本  </p>
<p><strong>此处错误：</strong><br>1、错误1：   </p>
<blockquote>
<p>Traceback (most recent call last):<br>  File “./python/draw_net.py”, line 6, in &lt;\module&gt;<br>    from google.protobuf import text_format<br>ImportError: No module named google.protobuf   </p>
</blockquote>
<p>解决方法：   </p>
<blockquote>
<p>sudo apt-get install python-protobuf  </p>
</blockquote>
<p>2、错误2：<br><figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> zhouyang@zy-ubunut-<span class="symbol">desktop:</span>~\/caffe-master$./python/draw_net.py ./examples/mnist/lenet_train_test.prototxt ~\/桌面/lenet_train_test.png</span><br><span class="line">Traceback (most recent call last)<span class="symbol">:</span></span><br><span class="line">  File <span class="string">"./python/draw_net.py"</span>, line <span class="number">8</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></span><br><span class="line">    import caffe</span><br><span class="line">  File <span class="string">"/home/zhouyang/caffe-master/python/caffe/__init__.py"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></span><br><span class="line">    from .pycaffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, RMSPropSolver, AdaDeltaSolver, AdamSolver, NCCL, Timer</span><br><span class="line">  File <span class="string">"/home/zhouyang/caffe-master/python/caffe/pycaffe.py"</span>, line <span class="number">13</span>, <span class="keyword">in</span> &lt;<span class="class"><span class="keyword">module</span>&gt;</span></span><br><span class="line">    from ._caffe import Net, SGDSolver, NesterovSolver, AdaGradSolver, \</span><br><span class="line"><span class="symbol">ImportError:</span> No <span class="class"><span class="keyword">module</span> <span class="title">named</span> <span class="title">_caffe</span></span></span><br><span class="line"></span><br><span class="line"><span class="string">`</span></span><br></pre></td></tr></table></figure></p>
<p>解决方法：<br>用caffe绘制网络图出现这个错误，是因为编译的时候，没有开启Makefile.config的这个选项：  </p>
<blockquote>
<p># Uncomment to support layers written in Python (will link against Python libs)<br>WITH_PYTHON_LAYER := 1  # 使用此行即可，然后重新编译caffe</p>
</blockquote>
<p>不更新程序，make会出现make:没有什么可以做’all’,需要make clean。</p>
<p>随后，重新编译caffe<br>~\/zhouyang/caffe-master# make all -j6</p>
<blockquote>
<p># in caffe root dir<br>make all<br>make pycaffe<br>make distribute<br># make dir for custom python modules, install caffe<br>mkdir ~\/python<br>mv distribute/python/caffe ~\/python<br># set PYTHONPATH (this should go in your .bashrc or whatever<br>PYTHONPATH=\${HOME}/python:\$PYTHONPATH   </p>
</blockquote>
<p>或 也可以  </p>
<blockquote>
<p>PYTHONPATH=\/home/zhouyang/caffe-master/distribute/python:$PYTHONPATH</p>
</blockquote>
<blockquote>
<p>sudo apt install graphviz  </p>
</blockquote>
<blockquote>
<p>sudo apt-get install python-pydot   </p>
</blockquote>
<p><strong>解决错误后绘制网络结构</strong>   </p>
<blockquote>
<p>zhouyang@zy-ubuntu-desktop:~\/caffe-master$ ./python/draw_net.py ./examples/mnist/lenet_train_test.prototxt ~\/桌面/lenet_train_test.png   </p>
</blockquote>
<h1 id="配置网络求解文件"><a href="#配置网络求解文件" class="headerlink" title="配置网络求解文件"></a>配置网络求解文件</h1><h1 id="官方Define-the-MNIST-Network说明"><a href="#官方Define-the-MNIST-Network说明" class="headerlink" title="官方Define the MNIST Network说明"></a>官方Define the MNIST Network说明</h1><p>This section explains the <code>lenet_train_test.prototxt</code> model definition that specifies the LeNet model for MNIST handwritten digit classification. We assume that you are familiar with <a href="https:\//developers.google.com/protocol-buffers/docs/overview" target="_blank" rel="noopener"><strong>Google Protobuf</strong></a>, and assume that you have read the protobuf definitions used by Caffe, which can be found at <code>$CAFFE_ROOT/src/caffe/proto/caffe.proto</code>.</p>
<p>Specifically, we will write a <code>caffe::NetParameter</code> (or in python, <code>caffe.proto.caffe_pb2.NetParameter</code>) protobuf. We will start by giving the network a name:</p>
<pre><code>name: &quot;LeNet&quot;
</code></pre><h3 id="Writing-the-Data-Layer"><a href="#Writing-the-Data-Layer" class="headerlink" title="Writing the Data Layer"></a>Writing the Data Layer</h3><p>Currently, we will read the MNIST data from the lmdb we created earlier in the demo. This is defined by a data layer:</p>
<pre><code>layer {
  name: &quot;mnist&quot;
  type: &quot;Data&quot;
  transform_param {
    scale: 0.00390625
  }
  data_param {
    source: &quot;mnist_train_lmdb&quot;
    backend: LMDB
    batch_size: 64
  }
  top: &quot;data&quot;
  top: &quot;label&quot;
}
</code></pre><p>Specifically, this layer has name <code>mnist</code>, type <code>data</code>, and it reads the data from the given lmdb source. We will use a batch size of 64, and scale the incoming pixels so that they are in the range [0,1). Why 0.00390625? It is 1 divided by 256. And finally, this layer produces two blobs, one is the <code>data</code> blob, and one is the <code>label</code> blob.</p>
<h3 id="Writing-the-Convolution-Layer"><a href="#Writing-the-Convolution-Layer" class="headerlink" title="Writing the Convolution Layer"></a>Writing the Convolution Layer</h3><p>Let’s define the first convolution layer:</p>
<pre><code>layer {
  name: &quot;conv1&quot;
  type: &quot;Convolution&quot;
  param { lr_mult: 1 }
  param { lr_mult: 2 }
  convolution_param {
    num_output: 20
    kernel_size: 5
    stride: 1
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
  bottom: &quot;data&quot;
  top: &quot;conv1&quot;
}
</code></pre><p>This layer takes the <code>data</code> blob (it is provided by the data layer), and produces the <code>conv1</code> layer. It produces outputs of 20 channels, with the convolutional kernel size 5 and carried out with stride 1.</p>
<p>The fillers allow us to randomly initialize the value of the weights and bias. For the weight filler, we will use the <code>xavier</code> algorithm that automatically determines the scale of initialization based on the number of input and output neurons. For the bias filler, we will simply initialize it as constant, with the default filling value 0.</p>
<p><code>lr_mult</code>s are the learning rate adjustments for the layer’s learnable parameters. In this case, we will set the weight learning rate to be the same as the learning rate given by the solver during runtime, and the bias learning rate to be twice as large as that - this usually leads to better convergence rates.</p>
<h3 id="Writing-the-Pooling-Layer"><a href="#Writing-the-Pooling-Layer" class="headerlink" title="Writing the Pooling Layer"></a>Writing the Pooling Layer</h3><p>Phew. Pooling layers are actually much easier to define:</p>
<pre><code>layer {
  name: &quot;pool1&quot;
  type: &quot;Pooling&quot;
  pooling_param {
    kernel_size: 2
    stride: 2
    pool: MAX
  }
  bottom: &quot;conv1&quot;
  top: &quot;pool1&quot;
}
</code></pre><p>This says we will perform max pooling with a pool kernel size 2 and a stride of 2 (so no overlapping between neighboring pooling regions).</p>
<p>Similarly, you can write up the second convolution and pooling layers. Check <code>$CAFFE_ROOT/examples/mnist/lenet_train_test.prototxt</code> for details.</p>
<h3 id="Writing-the-Fully-Connected-Layer"><a href="#Writing-the-Fully-Connected-Layer" class="headerlink" title="Writing the Fully Connected Layer"></a>Writing the Fully Connected Layer</h3><p>Writing a fully connected layer is also simple:</p>
<pre><code>layer {
  name: &quot;ip1&quot;
  type: &quot;InnerProduct&quot;
  param { lr_mult: 1 }
  param { lr_mult: 2 }
  inner_product_param {
    num_output: 500
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
  bottom: &quot;pool2&quot;
  top: &quot;ip1&quot;
}
</code></pre><p>This defines a fully connected layer (known in Caffe as an <code>InnerProduct</code> layer) with 500 outputs. All other lines look familiar, right?</p>
<h3 id="Writing-the-ReLU-Layer"><a href="#Writing-the-ReLU-Layer" class="headerlink" title="Writing the ReLU Layer"></a>Writing the ReLU Layer</h3><p>A ReLU Layer is also simple:</p>
<pre><code>layer {
  name: &quot;relu1&quot;
  type: &quot;ReLU&quot;
  bottom: &quot;ip1&quot;
  top: &quot;ip1&quot;
}
</code></pre><p>Since ReLU is an element-wise operation, we can do <em>in-place</em> operations to save some memory. This is achieved by simply giving the same name to the bottom and top blobs. Of course, do NOT use duplicated blob names for other layer types!</p>
<p>After the ReLU layer, we will write another innerproduct layer:</p>
<pre><code>layer {
  name: &quot;ip2&quot;
  type: &quot;InnerProduct&quot;
  param { lr_mult: 1 }
  param { lr_mult: 2 }
  inner_product_param {
    num_output: 10
    weight_filler {
      type: &quot;xavier&quot;
    }
    bias_filler {
      type: &quot;constant&quot;
    }
  }
  bottom: &quot;ip1&quot;
  top: &quot;ip2&quot;
}
</code></pre><h3 id="Writing-the-Loss-Layer"><a href="#Writing-the-Loss-Layer" class="headerlink" title="Writing the Loss Layer"></a>Writing the Loss Layer</h3><p>Finally, we will write the loss!</p>
<pre><code>layer {
  name: &quot;loss&quot;
  type: &quot;SoftmaxWithLoss&quot;
  bottom: &quot;ip2&quot;
  bottom: &quot;label&quot;
}
</code></pre><p>The <code>softmax_loss</code> layer implements both the softmax and the multinomial logistic loss (that saves time and improves numerical stability). It takes two blobs, the first one being the prediction and the second one being the <code>label</code> provided by the data layer (remember it?). It does not produce any outputs - all it does is to compute the loss function value, report it when backpropagation starts, and initiates the gradient with respect to <code>ip2</code>. This is where all magic starts.</p>
<h3 id="Additional-Notes-Writing-Layer-Rules"><a href="#Additional-Notes-Writing-Layer-Rules" class="headerlink" title="Additional Notes: Writing Layer Rules"></a>Additional Notes: Writing Layer Rules</h3><p>Layer definitions can include rules for whether and when they are included in the network definition, like the one below:</p>
<pre><code>layer {
  // ...layer definition...
  include: { phase: TRAIN }
}
</code></pre><p>This is a rule, which controls layer inclusion in the network, based on current network’s state.<br>You can refer to <code>$CAFFE_ROOT/src/caffe/proto/caffe.proto</code> for more information about layer rules and model schema.</p>
<p>In the above example, this layer will be included only in <code>TRAIN</code> phase.<br>If we change <code>TRAIN</code> with <code>TEST</code>, then this layer will be used only in test phase.<br>By default, that is without layer rules, a layer is always included in the network.<br>Thus, <code>lenet_train_test.prototxt</code> has two <code>DATA</code> layers defined (with different <code>batch_size</code>), one for the training phase and one for the testing phase.<br>Also, there is an <code>Accuracy</code> layer which is included only in <code>TEST</code> phase for reporting the model accuracy every 100 iteration, as defined in <code>lenet_solver.prototxt</code>.</p>
<h2 id="Define-the-MNIST-Solver"><a href="#Define-the-MNIST-Solver" class="headerlink" title="Define the MNIST Solver"></a>Define the MNIST Solver</h2><p>Check out the comments explaining each line in the prototxt <code>$CAFFE_ROOT/examples/mnist/lenet_solver.prototxt</code>:</p>
<pre><code># The train/test net protocol buffer definition
net: &quot;examples/mnist/lenet_train_test.prototxt&quot;
# test_iter specifies how many forward passes the test should carry out.
# In the case of MNIST, we have test batch size 100 and 100 test iterations,
# covering the full 10,000 testing images.
test_iter: 100
# Carry out testing every 500 training iterations.
test_interval: 500
# The base learning rate, momentum and the weight decay of the network.
base_lr: 0.01
momentum: 0.9
weight_decay: 0.0005
# The learning rate policy
lr_policy: &quot;inv&quot;
gamma: 0.0001
power: 0.75
# Display every 100 iterations
display: 100
# The maximum number of iterations
max_iter: 10000
# snapshot intermediate results
snapshot: 5000
snapshot_prefix: &quot;examples/mnist/lenet&quot;
# solver mode: CPU or GPU
solver_mode: GPU
</code></pre><h2 id="Training-and-Testing-the-Model"><a href="#Training-and-Testing-the-Model" class="headerlink" title="Training and Testing the Model"></a>Training and Testing the Model</h2><p>Training the model is simple after you have written the network definition protobuf and solver protobuf files. Simply run <code>train_lenet.sh</code>, or the following command directly:</p>
<pre><code>cd $CAFFE_ROOT
./examples/mnist/train_lenet.sh
</code></pre><p><code>train_lenet.sh</code> is a simple script, but here is a quick explanation: the main tool for training is <code>caffe</code> with action <code>train</code> and the solver protobuf text file as its argument.</p>
<p>When you run the code, you will see a lot of messages flying by like this:</p>
<pre><code>I1203 net.cpp:66] Creating Layer conv1
I1203 net.cpp:76] conv1 &lt;- data
I1203 net.cpp:101] conv1 -&gt; conv1
I1203 net.cpp:116] Top shape: 20 24 24
I1203 net.cpp:127] conv1 needs backward computation.
</code></pre><p>These messages tell you the details about each layer, its connections and its output shape, which may be helpful in debugging. After the initialization, the training will start:</p>
<pre><code>I1203 net.cpp:142] Network initialization done.
I1203 solver.cpp:36] Solver scaffolding done.
I1203 solver.cpp:44] Solving LeNet
</code></pre><p>Based on the solver setting, we will print the training loss function every 100 iterations, and test the network every 500 iterations. You will see messages like this:</p>
<pre><code>I1203 solver.cpp:204] Iteration 100, lr = 0.00992565
I1203 solver.cpp:66] Iteration 100, loss = 0.26044
...
I1203 solver.cpp:84] Testing net
I1203 solver.cpp:111] Test score #0: 0.9785
I1203 solver.cpp:111] Test score #1: 0.0606671
</code></pre><p>For each training iteration, <code>lr</code> is the learning rate of that iteration, and <code>loss</code> is the training function. For the output of the testing phase, score 0 is the accuracy, and score 1 is the testing loss function.</p>
<p>And after a few minutes, you are done!</p>
<pre><code>I1203 solver.cpp:84] Testing net
I1203 solver.cpp:111] Test score #0: 0.9897
I1203 solver.cpp:111] Test score #1: 0.0324599
I1203 solver.cpp:126] Snapshotting to lenet_iter_10000
I1203 solver.cpp:133] Snapshotting solver state to lenet_iter_10000.solverstate
I1203 solver.cpp:78] Optimization Done.
</code></pre><p>The final model, stored as a binary protobuf file, is stored at</p>
<pre><code>lenet_iter_10000
</code></pre><p>which you can deploy as a trained model in your application, if you are training on a real-world application dataset.</p>
<h3 id="Um…-How-about-GPU-training"><a href="#Um…-How-about-GPU-training" class="headerlink" title="Um… How about GPU training?"></a>Um… How about GPU training?</h3><p>You just did! All the training was carried out on the GPU. In fact, if you would like to do training on CPU, you can simply change one line in <code>lenet_solver.prototxt</code>:</p>
<pre><code># solver mode: CPU or GPU
solver_mode: CPU
</code></pre><p>and you will be using CPU for training. Isn’t that easy?</p>
<p>MNIST is a small dataset, so training with GPU does not really introduce too much benefit due to communication overheads. On larger datasets with more complex models, such as ImageNet, the computation speed difference will be more significant.</p>
<h3 id="How-to-reduce-the-learning-rate-at-fixed-steps"><a href="#How-to-reduce-the-learning-rate-at-fixed-steps" class="headerlink" title="How to reduce the learning rate at fixed steps?"></a>How to reduce the learning rate at fixed steps?</h3><p>Look at lenet_multistep_solver.prototxt</p>

      
    </div>
    
    
    

    

    
      <div>
        <div style="padding: 10px 0; margin: 20px auto; width: 90%; text-align: center;">
  <div>支持我吧</div>
  <button id="rewardButton" disable="enable" onclick="var qr = document.getElementById('QR'); if (qr.style.display === 'none') {qr.style.display='block';} else {qr.style.display='none'}">
    <span>打赏</span>
  </button>
  <div id="QR" style="display: none;">

    
      <div id="wechat" style="display: inline-block">
        <img id="wechat_qr" src="/images/wechatpay.bmp" alt="未晞 微信支付"/>
        <p>微信支付</p>
      </div>
    

    
      <div id="alipay" style="display: inline-block">
        <img id="alipay_qr" src="/images/alipay.bmp" alt="未晞 支付宝"/>
        <p>支付宝</p>
      </div>
    

    

  </div>
</div>

      </div>
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Caffe/" rel="tag"># Caffe</a>
          
            <a href="/tags/Ubuntu/" rel="tag"># Ubuntu</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/12/21/Ubuntu从入门到精通——5 编译配置第三方库Caffe/" rel="next" title="Ubuntu从入门到精通——5 编译配置第三方库Caffe">
                <i class="fa fa-chevron-left"></i> Ubuntu从入门到精通——5 编译配置第三方库Caffe
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/12/23/Caffe学习——2-Protocol-Buffers学习/" rel="prev" title="Caffe学习——2 Protocol Buffers学习">
                Caffe学习——2 Protocol Buffers学习 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
      <div id="sidebar-dimmer"></div>
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <img class="site-author-image" itemprop="image"
                src="/images/tuxiang.png"
                alt="未晞" />
            
              <p class="site-author-name" itemprop="name">未晞</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives">
              
                  <span class="site-state-item-count">25</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-categories">
                <a href="/categories/index.html">
                  <span class="site-state-item-count">13</span>
                  <span class="site-state-item-name">分类</span>
                </a>
              </div>
            

            
              
              
              <div class="site-state-item site-state-tags">
                <a href="/tags/index.html">
                  <span class="site-state-item-count">43</span>
                  <span class="site-state-item-name">标签</span>
                </a>
              </div>
            

          </nav>

          

          
            <div class="links-of-author motion-element">
                
                  <span class="links-of-author-item">
                    <a href="https://github.com/weixi234" target="_blank" title="GitHub">
                      
                        <i class="fa fa-fw fa-github"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="https://weibo.com/3598071457" target="_blank" title="Weibo">
                      
                        <i class="fa fa-fw fa-weibo"></i></a>
                  </span>
                
                  <span class="links-of-author-item">
                    <a href="mailto:zhouyang234@hotmail.com" target="_blank" title="E-Mail">
                      
                        <i class="fa fa-fw fa-envelope"></i></a>
                  </span>
                
            </div>
          

          
          

          
          

          

        </div>
      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#前言"><span class="nav-number">1.</span> <span class="nav-text">前言</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#Caffe及LeNetMnist相关说明"><span class="nav-number">2.</span> <span class="nav-text">Caffe及LeNetMnist相关说明</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#准备数据"><span class="nav-number">3.</span> <span class="nav-text">准备数据</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#配置网络模型"><span class="nav-number">4.</span> <span class="nav-text">配置网络模型</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#配置网络求解文件"><span class="nav-number">5.</span> <span class="nav-text">配置网络求解文件</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#官方Define-the-MNIST-Network说明"><span class="nav-number">6.</span> <span class="nav-text">官方Define the MNIST Network说明</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-the-Data-Layer"><span class="nav-number">6.0.1.</span> <span class="nav-text">Writing the Data Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-the-Convolution-Layer"><span class="nav-number">6.0.2.</span> <span class="nav-text">Writing the Convolution Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-the-Pooling-Layer"><span class="nav-number">6.0.3.</span> <span class="nav-text">Writing the Pooling Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-the-Fully-Connected-Layer"><span class="nav-number">6.0.4.</span> <span class="nav-text">Writing the Fully Connected Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-the-ReLU-Layer"><span class="nav-number">6.0.5.</span> <span class="nav-text">Writing the ReLU Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Writing-the-Loss-Layer"><span class="nav-number">6.0.6.</span> <span class="nav-text">Writing the Loss Layer</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Additional-Notes-Writing-Layer-Rules"><span class="nav-number">6.0.7.</span> <span class="nav-text">Additional Notes: Writing Layer Rules</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Define-the-MNIST-Solver"><span class="nav-number">6.1.</span> <span class="nav-text">Define the MNIST Solver</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Training-and-Testing-the-Model"><span class="nav-number">6.2.</span> <span class="nav-text">Training and Testing the Model</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Um…-How-about-GPU-training"><span class="nav-number">6.2.1.</span> <span class="nav-text">Um… How about GPU training?</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#How-to-reduce-the-learning-rate-at-fixed-steps"><span class="nav-number">6.2.2.</span> <span class="nav-text">How to reduce the learning rate at fixed steps?</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; 2018 &mdash; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">晞</span>

  
</div>








  <div class="footer-custom">白露未晞，看书学习到天明</div>



        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  

  
  
    <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.4"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.4"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  







<!-- LOCAL: You can save these files to your site and update links -->
    
        
        <link rel="stylesheet" href="https://aimingoo.github.io/gitmint/style/default.css">
        <script src="https://aimingoo.github.io/gitmint/dist/gitmint.browser.js"></script>
    
<!-- END LOCAL -->

    
      <style>
        a.gitment-editor-footer-tip { display: none; }
        .gitment-container.gitment-footer-container { display: none; }
      </style>
    

    
      <script type="text/javascript">
      function renderGitment(){
        var gitment = new Gitmint({
            id: window.location.pathname, 
            owner: 'weixi234',
            repo: 'weixi234.github.io',
            
            lang: "" || navigator.language || navigator.systemLanguage || navigator.userLanguage,
            
            oauth: {
            
            
                client_secret: '8bde11549e9e28c95a421229b406bd28f5ac18ca',
            
                client_id: '00330979e367fd9e4fa0'
            }});
        gitment.render('gitment-container');
      }

      
      renderGitment();
      
      </script>
    







  





  

  

  
<script>
(function(){
    var bp = document.createElement('script');
    var curProtocol = window.location.protocol.split(':')[0];
    if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';        
    }
    else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
    }
    var s = document.getElementsByTagName("script")[0];
    s.parentNode.insertBefore(bp, s);
})();
</script>


  
  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
  


  

  

</body>
</html>
